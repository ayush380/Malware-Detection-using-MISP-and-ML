import pandas as pd
import numpy as nu
import sklearn.ensemble as skensemble
from sklearn.feature_selection import SelectFromModel
from sklearn import cross_validation , tree , linear_model
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
import pefile 
import argparse
import hashlib
import array
import sys
import math
import csv
import os
import virustotalextractor as vs
import mispextractor as mi
import pickle
import update as ud
import ntpath
from sklearn.externals import joblib
data = pd.read_csv('data.csv',sep='|')
def prelim(fname):
  #for signature based detection  
    print("Conducting preliminary test")
    hash_md5 = hashlib.md5()
    with open(fname,"rb") as f:
        for chunk in iter(lambda: f.read(4096),b""):
            hash_md5.update(chunk)
    md5=hash_md5.hexdigest()        
   
    x=data.loc[data['md5']==md5]
    if x.empty:
        print("")
        print("not found in database, predicting")
        print("")
        flag=0
    else:
        if(x['legitimate'].values[0]==0):
            print("malicious")
        else:
            print("legit")
	    
        flag=1
    retu={
        "flag":flag,
        "md5":md5
        }

    return retu
def get_entropy(data):
    if len(data) == 0:
        return 0
    occurences = array.array('L', [0]*256)
    for x in data:
        occurences[x if isinstance(x, int) else ord(x)] += 1

    entropy = 0
    for x in occurences:
        if x:
            p_x = float(x) / len(data)
            entropy -= p_x*math.log(p_x, 2)

    return entropy

def get_resources(pe):
    """Extract resources :
    [entropy, size]"""
    resources = []
    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
        try:
            
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(resource_type, 'directory'):
                    for resource_id in resource_type.directory.entries:
                        if hasattr(resource_id, 'directory'):
                            for resource_lang in resource_id.directory.entries:
                                data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)
                                size = resource_lang.data.struct.Size
                                entropy = get_entropy(data)

                                resources.append([entropy, size])
        except Exception as e:
            return resources
    return resources

def get_version_info(pe):
    """Return version infos"""
    res = {}
    for fileinfo in pe.FileInfo:
        if fileinfo.Key == 'StringFileInfo':
            for st in fileinfo.StringTable:
                for entry in st.entries.items():
                    res[entry[0]] = entry[1]
        if fileinfo.Key == 'VarFileInfo':
            for var in fileinfo.Var:
                res[var.entry.items()[0][0]] = var.entry.items()[0][1]
    if hasattr(pe, 'VS_FIXEDFILEINFO'):
          res['flags'] = pe.VS_FIXEDFILEINFO.FileFlags
          res['os'] = pe.VS_FIXEDFILEINFO.FileOS
          res['type'] = pe.VS_FIXEDFILEINFO.FileType
          res['file_version'] = pe.VS_FIXEDFILEINFO.FileVersionLS
          res['product_version'] = pe.VS_FIXEDFILEINFO.ProductVersionLS
          res['signature'] = pe.VS_FIXEDFILEINFO.Signature
          res['struct_version'] = pe.VS_FIXEDFILEINFO.StrucVersion
    return res

def extract_infos(fpath):
    res = {}
    pe = pefile.PE(fpath)
    res['Machine'] = pe.FILE_HEADER.Machine
    res['SizeOfOptionalHeader'] = pe.FILE_HEADER.SizeOfOptionalHeader
    res['Characteristics'] = pe.FILE_HEADER.Characteristics
    res['MajorLinkerVersion'] = pe.OPTIONAL_HEADER.MajorLinkerVersion
    res['MinorLinkerVersion'] = pe.OPTIONAL_HEADER.MinorLinkerVersion
    res['SizeOfCode'] = pe.OPTIONAL_HEADER.SizeOfCode
    res['SizeOfInitializedData'] = pe.OPTIONAL_HEADER.SizeOfInitializedData
    res['SizeOfUninitializedData'] = pe.OPTIONAL_HEADER.SizeOfUninitializedData
    res['AddressOfEntryPoint'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint
    res['BaseOfCode'] = pe.OPTIONAL_HEADER.BaseOfCode
    try:
        res['BaseOfData'] = pe.OPTIONAL_HEADER.BaseOfData
    except AttributeError:
        res['BaseOfData'] = 0
    res['ImageBase'] = pe.OPTIONAL_HEADER.ImageBase
    res['SectionAlignment'] = pe.OPTIONAL_HEADER.SectionAlignment
    res['FileAlignment'] = pe.OPTIONAL_HEADER.FileAlignment
    res['MajorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
    res['MinorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion
    res['MajorImageVersion'] = pe.OPTIONAL_HEADER.MajorImageVersion
    res['MinorImageVersion'] = pe.OPTIONAL_HEADER.MinorImageVersion
    res['MajorSubsystemVersion'] = pe.OPTIONAL_HEADER.MajorSubsystemVersion
    res['MinorSubsystemVersion'] = pe.OPTIONAL_HEADER.MinorSubsystemVersion
    res['SizeOfImage'] = pe.OPTIONAL_HEADER.SizeOfImage
    res['SizeOfHeaders'] = pe.OPTIONAL_HEADER.SizeOfHeaders
    res['CheckSum'] = pe.OPTIONAL_HEADER.CheckSum
    res['Subsystem'] = pe.OPTIONAL_HEADER.Subsystem
    res['DllCharacteristics'] = pe.OPTIONAL_HEADER.DllCharacteristics
    res['SizeOfStackReserve'] = pe.OPTIONAL_HEADER.SizeOfStackReserve
    res['SizeOfStackCommit'] = pe.OPTIONAL_HEADER.SizeOfStackCommit
    res['SizeOfHeapReserve'] = pe.OPTIONAL_HEADER.SizeOfHeapReserve
    res['SizeOfHeapCommit'] = pe.OPTIONAL_HEADER.SizeOfHeapCommit
    res['LoaderFlags'] = pe.OPTIONAL_HEADER.LoaderFlags
    res['NumberOfRvaAndSizes'] = pe.OPTIONAL_HEADER.NumberOfRvaAndSizes

    # Sections
    res['SectionsNb'] = len(pe.sections)
    entropy = list(map(lambda x:x.get_entropy(), pe.sections))
    res['SectionsMeanEntropy'] = sum(entropy)/float(len(entropy))
    res['SectionsMinEntropy'] = min(entropy)
    res['SectionsMaxEntropy'] = max(entropy)
    raw_sizes = list(map(lambda x:x.SizeOfRawData, pe.sections))
    res['SectionsMeanRawsize'] = sum(raw_sizes)/float(len(raw_sizes))
    res['SectionsMinRawsize'] = min(raw_sizes)
    res['SectionsMaxRawsize'] = max(raw_sizes)
    virtual_sizes = list(map(lambda x:x.Misc_VirtualSize, pe.sections))
    res['SectionsMeanVirtualsize'] = sum(virtual_sizes)/float(len(virtual_sizes))
    res['SectionsMinVirtualsize'] = min(virtual_sizes)
    res['SectionMaxVirtualsize'] = max(virtual_sizes)

    #Imports
    try:
        res['ImportsNbDLL'] = len(pe.DIRECTORY_ENTRY_IMPORT)
        imports = sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], [])
        res['ImportsNb'] = len(imports)
        res['ImportsNbOrdinal'] = len(list(filter(lambda x:x.name is None, imports)))
    except AttributeError:
        res['ImportsNbDLL'] = 0
        res['ImportsNb'] = 0
        res['ImportsNbOrdinal'] = 0

    #Exports
    try:
        res['ExportNb'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)
    except AttributeError:
        # No export
        res['ExportNb'] = 0
    #Resources
    resources= get_resources(pe)
    res['ResourcesNb'] = len(resources)
    if len(resources)> 0:
        entropy = list(map(lambda x:x[0], resources))
        res['ResourcesMeanEntropy'] = sum(entropy)/float(len(entropy))
        res['ResourcesMinEntropy'] = min(entropy)
        res['ResourcesMaxEntropy'] = max(entropy)
        sizes =list( map(lambda x:x[1], resources))
        res['ResourcesMeanSize'] = sum(sizes)/float(len(sizes))
        res['ResourcesMinSize'] = min(sizes)
        res['ResourcesMaxSize'] = max(sizes)
    else:
        res['ResourcesNb'] = 0
        res['ResourcesMeanEntropy'] = 0
        res['ResourcesMinEntropy'] = 0
        res['ResourcesMaxEntropy'] = 0
        res['ResourcesMeanSize'] = 0
        res['ResourcesMinSize'] = 0
        res['ResourcesMaxSize'] = 0

    # Load configuration size
    try:
        res['LoadConfigurationSize'] = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size
    except AttributeError:
        res['LoadConfigurationSize'] = 0


    # Version configuration size
    try:
        version_infos = get_version_info(pe)
        res['VersionInformationSize'] = len(version_infos.keys())
    except AttributeError:
        res['VersionInformationSize'] = 0
    return res    
    
def learning():
    
    
    X=data.drop(['Name','md5','legitimate'], axis=1).values

    # To drop non-affecting columns so as to prevent overfit

    y=data['legitimate'].values

    # y is the target variable for machine learning

    #Selection of features using trees classfier

    rdtree= skensemble.ExtraTreesClassifier().fit(X,y)

    #rdtree is a collection of random decision trees

    model=SelectFromModel(rdtree,prefit=True)

    #SelectFromModel chooses the best feature from the random decision trees

    X_new =model.transform(X)
    #X_new contains useful features
    samples=X_new.shape[1]

    X_train, X_test, y_train, y_test= cross_validation.train_test_split(X_new,y,test_size=0.2)

    #train_test_split function splits the dataset into training and testing datasets

    #X_train has features for training our model

    #X_test has features for testing our model

    #y_train has the target for training the model

    #y_test has the features for testing the model

    features=[]
    indices=nu.argsort(rdtree.feature_importances_)[::-1][:samples]
    for f in range(samples):
        print("%d feature %s (%f)" % (f+1, data.columns[2+indices[f]],rdtree.feature_importances_[indices[f]]))

    for f in sorted(nu.argsort(rdtree.feature_importances_)[::-1][:samples]):
           features.append(data.columns[2+f])
           algorithms={
               "DecisionTree":tree.DecisionTreeClassifier(max_depth=10),
               "RandomForest":skensemble.RandomForestClassifier(n_estimators=50),
               "GradientBoosting":skensemble.GradientBoostingClassifier(n_estimators=50),
               "AdaBoost":skensemble.AdaBoostClassifier(n_estimators=50),
               # "SVM":svm.SVC(),
               #"KNeighbor":KNeighborsClassifier(),
               "Xgb":XGBClassifier()
           }
    results={}
    ans={}
    for algo in algorithms:
       mdl=algorithms[algo]
       print("training with %s" % (algo)) 
       ans[algo]=mdl.fit(X_train,y_train)
       score=mdl.score(X_test,y_test)
       print("%s:%f "%(algo,score))
       results[algo]=score
   

    winner=max(results,key=results.get)
    clf=ans[winner]
    joblib.dump(algorithms[winner], 'classifier/classifier.pkl')
    open('classifier/features.pkl', 'wb').write(pickle.dumps(features))
  
def feedback(fname,md5,res,legit):
    struc="\n"+fname+"|"+str(md5)+"|"+str(res['Machine'])+"|"+str(res['SizeOfOptionalHeader'])+"|"+str(res['Characteristics'])+"|"+str(res['MajorLinkerVersion'])+"|"+str(res['MinorLinkerVersion'])+"|"+str(res['SizeOfCode'])+"|"+str(res['SizeOfInitializedData'])+"|"+str(res['SizeOfUninitializedData'])+"|"+str(res['AddressOfEntryPoint'])+"|"+str(res['BaseOfCode'])+"|"+str(res['BaseOfData'])+"|"+str(res['ImageBase'])+"|"+str(res['SectionAlignment'])+"|"+str(res['FileAlignment'])+"|"+str(res['MajorOperatingSystemVersion'])+"|"+str(res['MinorOperatingSystemVersion'])+"|"+str(res['MajorImageVersion'])+"|"+str(res['MinorImageVersion'])+"|"+str(res['MajorSubsystemVersion'])+"|"+str(res['MinorSubsystemVersion'])+"|"+str(res['SizeOfImage'])+"|"+str(res['SizeOfHeaders'])+"|"+str(res['CheckSum'])+"|"+str(res['Subsystem'])+"|"+str(res['DllCharacteristics'])+"|"+str(res['SizeOfStackReserve'])+"|"+str(res['SizeOfStackCommit'])+"|"+str(res['SizeOfHeapReserve'])+"|"+str(res['SizeOfHeapCommit'])+"|"+str(res['LoaderFlags'])+"|"+str(res['NumberOfRvaAndSizes'])+"|"+str(res['SectionsNb'])+"|"+str(res['SectionsMeanEntropy'])+"|"+str(res['SectionsMinEntropy'])+"|"+str(res['SectionsMaxEntropy'])+"|"+str(res['SectionsMeanRawsize'])+"|"+str(res['SectionsMinRawsize'])+"|"+str(res['SectionsMaxRawsize'])+"|"+str(res['SectionsMeanVirtualsize'])+"|"+str(res['SectionsMinVirtualsize'])+"|"+str(res['SectionMaxVirtualsize'])+"|"+str(res['ImportsNbDLL'])+"|"+str(res['ImportsNbDLL'])+"|"+str(0)+"|"+str(res['ExportNb'])+"|"+str(res['ResourcesNb'])+"|"+str(res['ResourcesMeanEntropy'])+"|"+str(res['ResourcesMinEntropy'])+"|"+str(res['ResourcesMaxEntropy'])+"|"+str(res['ResourcesMeanSize'])+"|"+str(res['ResourcesMinSize'])+"|"+str(res['ResourcesMaxSize'])+"|"+str(res['LoadConfigurationSize'])+"|"+str(res['VersionInformationSize'])+"|"+str(legit)
    fd=open('data.csv','a')
    fd.write(struc)
    fd.close()

        
   

    
def driver(fpath):
    ud.driver()
    vtflag=-1
    mispflag=-1
    lol=0
    
    b=prelim(fpath)
########################################
    if b["flag"]==0:
########################################
        vtr=vs.virusTotalExtractor(fpath)
        if vtr["connection"]==True:

            vtflag=vtr["positives"]

            #print("VTR")

            #print(vtflag)
        else:
            vtflag=-1
#########################################
        mispr=mi.mispextract(fpath)
        if mispr["connection"]==True:
            mispflag=mispr["positives"]

            #print("MISP")
            #print(mispflag) 
        else:
            mispflag=-1
##########################################        
        clf = joblib.load(os.path.join(os.path.dirname(os.path.realpath(__file__)),'classifier/classifier.pkl'))
        features = pickle.loads(open(os.path.join(os.path.dirname(os.path.realpath(__file__)),'classifier/features.pkl'),'rb').read())
        d=extract_infos(fpath)
        feats=list(map(lambda x:d[x], features))
        #print(d['Machine'])
        resu=clf.predict([feats])
        if vtflag>10:
            print('malicious')
            lol=0
            feedback(ntpath.basename(fpath),b["md5"],d,lol)
        if (vtflag<10 and  mispflag>0):
            print(['malicious','legitimate'][resu[0]])
            lol=resu[0]
        if (vtflag==0 and mispflag==0):
            print('legitimate')
            lol=1
            feedback(ntpath.basename(fpath),b["md5"],d,lol)
        if (vtflag==-1 and mispflag==-1):
            print(['malicious','legitimate'][resu[0]])
        
        
            
